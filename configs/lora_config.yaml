model_name: "facebook/opt-350m"
batch_size: 16
learning_rate: 5e-5
num_epochs: 3
gradient_accumulation_steps: 1
warmup_steps: 100
weight_decay: 0.01
fp16: true
local_rank: -1

# LoRA specific parameters
lora_r: 8
lora_alpha: 32
lora_dropout: 0.05
target_modules: ["q_proj", "v_proj"]

deepspeed_config:
  train_batch_size: 16
  fp16:
    enabled: true
  optimizer:
    type: "AdamW"
    params:
      lr: 5e-5
      weight_decay: 0.01
  scheduler:
    type: "WarmupLR"
    params:
      warmup_min_lr: 0
      warmup_max_lr: 5e-5
      warmup_num_steps: 100
  gradient_accumulation_steps: 1
  zero_optimization:
    stage: 2
    offload_optimizer:
      device: "cpu" 